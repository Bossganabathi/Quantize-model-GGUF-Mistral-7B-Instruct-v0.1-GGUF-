{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ctransformers\n",
      "  Using cached ctransformers-0.2.27.tar.gz (376 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub in /scratch/miniconda3/envs/ganaquantize/lib/python3.11/site-packages (from ctransformers) (0.20.3)\n",
      "Collecting py-cpuinfo<10.0.0,>=9.0.0 (from ctransformers)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: filelock in /scratch/miniconda3/envs/ganaquantize/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /scratch/miniconda3/envs/ganaquantize/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in /scratch/miniconda3/envs/ganaquantize/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /scratch/miniconda3/envs/ganaquantize/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /scratch/miniconda3/envs/ganaquantize/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /scratch/miniconda3/envs/ganaquantize/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (4.6.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /scratch/miniconda3/envs/ganaquantize/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /scratch/miniconda3/envs/ganaquantize/lib/python3.11/site-packages (from requests->huggingface-hub->ctransformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /scratch/miniconda3/envs/ganaquantize/lib/python3.11/site-packages (from requests->huggingface-hub->ctransformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /scratch/miniconda3/envs/ganaquantize/lib/python3.11/site-packages (from requests->huggingface-hub->ctransformers) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/miniconda3/envs/ganaquantize/lib/python3.11/site-packages (from requests->huggingface-hub->ctransformers) (2023.5.7)\n",
      "Building wheels for collected packages: ctransformers\n",
      "  Building wheel for ctransformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ctransformers: filename=ctransformers-0.2.27-cp311-cp311-linux_x86_64.whl size=596023 sha256=e9b2057b566d6e83a08b8b8152de6b53a3b84d55e656a46494159186c6544338\n",
      "  Stored in directory: /home2/onpremllm/.cache/pip/wheels/97/4a/52/527e0ba9715318161f0f7a69f5e60d53b4a1afcd62e61d3347\n",
      "Successfully built ctransformers\n",
      "Installing collected packages: py-cpuinfo, ctransformers\n",
      "Successfully installed ctransformers-0.2.27 py-cpuinfo-9.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install ctransformers --no-binary ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2b2acf7174496e8a7d34d06f93660b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd501df02724cadbfee9b22a87f0454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876ed6a33b4d41beb9c78fabe802c105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f53ce0585743e3bd5499bdccbee2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral-7b-instruct-v0.1.Q4_K_M.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96b907437644f44aea98c50f4952e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8f0d158c404d50990decc9c5a67853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a5e74879784a28b7f8da593d739cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aceb3a9b099f405c8f1735dc883882f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load LLM and Tokenizer\n",
    "# Use `gpu_layers` to specify how many layers will be offloaded to the GPU.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "    model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "    model_type=\"mistral\", gpu_layers=50, hf=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\", use_fast=True, padding_side='left'\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the tokenizer's chat template to format each message\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a funny joke.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Why did the chicken cross the road? To get to the other side, but certainly, the other side was full of peril and danger, so it quickly returned from whence it came, forsooth!\"\n",
    "    },\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Tell me a funny joke. [/INST]Why did the chicken cross the road? To get to the other side, but certainly, the other side was full of peril and danger, so it quickly returned from whence it came, forsooth!</s>\n"
     ]
    }
   ],
   "source": [
    "# We will use the same prompt as we did originally\n",
    "outputs = pipe(prompt, max_new_tokens=256)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ganaquantize",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
